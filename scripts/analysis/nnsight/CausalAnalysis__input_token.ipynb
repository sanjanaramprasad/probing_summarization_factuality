{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dced8d7e-c017-49df-b17a-6aa5037ecb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nnsight import LanguageModel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a4cbddd-2a00-4299-b4ed-03d6acf7665b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>source</th>\n",
       "      <th>summary</th>\n",
       "      <th>annotated_spans</th>\n",
       "      <th>model</th>\n",
       "      <th>origin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Rachel Usher#REDDIT-83:flanul2-ul2</td>\n",
       "      <td>both me and my girlfriend participate in winte...</td>\n",
       "      <td>I waited outside the locker rooms for my girlf...</td>\n",
       "      <td>nobody</td>\n",
       "      <td>flanul2</td>\n",
       "      <td>REDDIT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                  id  \\\n",
       "0           0  Rachel Usher#REDDIT-83:flanul2-ul2   \n",
       "\n",
       "                                              source  \\\n",
       "0  both me and my girlfriend participate in winte...   \n",
       "\n",
       "                                             summary annotated_spans    model  \\\n",
       "0  I waited outside the locker rooms for my girlf...          nobody  flanul2   \n",
       "\n",
       "   origin  \n",
       "0  REDDIT  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genaudit_read_path = '/home/ramprasad.sa/probing_summarization_factuality/datasets/Genaudit_annotations.csv'\n",
    "df_genaudit = pd.read_csv(genaudit_read_path)\n",
    "df_genaudit.head()[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f37341a-2d42-40cd-b745-b48215f07aa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>source</th>\n",
       "      <th>summary</th>\n",
       "      <th>annotated_spans</th>\n",
       "      <th>model</th>\n",
       "      <th>origin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Rachel Usher#REDDIT-83:mistral7b-ul2</td>\n",
       "      <td>both me and my girlfriend participate in winte...</td>\n",
       "      <td>The document describes the experience of a hig...</td>\n",
       "      <td>participates&lt;sep&gt;with their girlfriend&lt;sep&gt;ends</td>\n",
       "      <td>mistral7b</td>\n",
       "      <td>REDDIT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                    id  \\\n",
       "4           4  Rachel Usher#REDDIT-83:mistral7b-ul2   \n",
       "\n",
       "                                              source  \\\n",
       "4  both me and my girlfriend participate in winte...   \n",
       "\n",
       "                                             summary  \\\n",
       "4  The document describes the experience of a hig...   \n",
       "\n",
       "                                   annotated_spans      model  origin  \n",
       "4  participates<sep>with their girlfriend<sep>ends  mistral7b  REDDIT  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'mistral7b'\n",
    "df_genaudit_mistral = df_genaudit[df_genaudit['model'] == model_name]\n",
    "df_genaudit_mistral.head()[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2833d878-d83d-4944-8c3d-f544b3a83bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'falcon7b', 'flanul2', 'mistral7b', 'llama70b', 'chatgpt', 'geminipro', 'llama7b', 'gpt4'}\n"
     ]
    }
   ],
   "source": [
    "print(set(df_genaudit['model']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ac6b402-c696-4a83-85fb-41c25748ae73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ramprasad.sa/.conda/envs/nnsight/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b1fead2fbea4fe2b557369b228cbf65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MistralRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm()\n",
       "        (post_attention_layernorm): MistralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "  (generator): WrapperModule()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model_path = {'mistral7b': 'mistralai/Mistral-7B-Instruct-v0.1',\n",
    "             'falcon7b': 'tiiuae/falcon-7b-instruct',\n",
    "             'llama7b': '/work/frink/models/Llama-2-7b-chat-hf',\n",
    "             'flanul2': 'google/flan-ul2'}\n",
    "\n",
    "def load_model(model_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path[model_name],\n",
    "                                         cache_dir = '/scratch/ramprasad.sa/huggingface_models')\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path[model_name],\n",
    "                                            cache_dir = '/scratch/ramprasad.sa/huggingface_models')\n",
    "    # model = model.to('cuda')\n",
    "    return tokenizer, model\n",
    "\n",
    "\n",
    "tokenizer, mistral_model = load_model(model_name)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "model = LanguageModel(mistral_model, tokenizer=tokenizer, device_map=\"auto\", dispatch=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "1973a380-3fd4-4ca4-a3d6-1744e33c80bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'instruction': 'Generate a summary for the following document in brief. When creating the summary, only use information that is present in the document',\n",
       "  'prompt_prefix_template': ' CONTENT: ',\n",
       "  'prompt_suffix_template': ' SUMMARY: '},\n",
       " '{instruction}{prompt_prefix}{source}{prompt_suffix}{summary}')"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json \n",
    "prompt_template = f'{{instruction}}{{prompt_prefix}}{{source}}{{prompt_suffix}}{{summary}}'\n",
    "\n",
    "prompt_template_path = '/home/ramprasad.sa/probing_summarization_factuality/datasets/prompt_templates/'\n",
    "prompt_type = 'document_context_causal'\n",
    "\n",
    "with open(f'{prompt_template_path}/{prompt_type}.json', 'r') as fp:\n",
    "    prompt_dict = json.load(fp)\n",
    "prompt_dict, prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d04219f-0916-46cf-b461-33767b2a8436",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def strip_bos_eos_ids(ids):\n",
    "    ids = ids[:, 1:] if ids[0][0] in [0,1,2] else ids\n",
    "    return ids\n",
    "    \n",
    "class PromptProcessor():\n",
    "\n",
    "    def __init__(self,\n",
    "                 prompt_template,\n",
    "                 prompt_dict_path,\n",
    "                prompt_type):\n",
    "\n",
    "        self.prompt_template = prompt_template\n",
    "\n",
    "        with open(f'{prompt_dict_path}/{prompt_type}.json', 'r') as fp:\n",
    "            self.prompt_dict = json.load(fp)\n",
    "            \n",
    "        self.instruction = self.prompt_dict['instruction'] if 'instruction' in self.prompt_dict else ''\n",
    "        self.prefix = self.prompt_dict['prompt_prefix_template'] if 'prompt_prefix_template' in self.prompt_dict else ''\n",
    "        self.suffix = self.prompt_dict['prompt_suffix_template'] if 'prompt_suffix_template' in self.prompt_dict else ''\n",
    "\n",
    "    def make_prompt(self,\n",
    "                    source,\n",
    "                    summary):\n",
    "        \n",
    "        prompt = self.prompt_template.format(instruction = self.instruction,\n",
    "                                    prompt_prefix = self.prefix,\n",
    "                                    source = source,\n",
    "                                    prompt_suffix = self.suffix,\n",
    "                                    summary = summary\n",
    "                                   )\n",
    "        return prompt\n",
    "\n",
    "\n",
    "    def get_prompt_attributes_idx(self,\n",
    "                                  prompt_ids,\n",
    "                                  source,\n",
    "                                  summary,\n",
    "                                  tokenizer):\n",
    "        instr_idx = -1\n",
    "        instr_prefix_idx = -1\n",
    "        instr_prefix_src_idx = -1\n",
    "        instr_prefix_src_suffix_idx = -1\n",
    "\n",
    "        for span_idx in range( len(prompt_ids)):\n",
    "        \n",
    "            span_tokens = prompt_ids[1:span_idx]\n",
    "            if tokenizer.decode(span_tokens) == self.prompt_template.format(instruction = self.instruction,\n",
    "                                            prompt_prefix = '',\n",
    "                                            source = '',\n",
    "                                            prompt_suffix = '',\n",
    "                                            summary = ''\n",
    "                                           ).strip():\n",
    "                instr_idx = span_idx\n",
    "            \n",
    "            if tokenizer.decode(span_tokens) == self.prompt_template.format(instruction = self.instruction,\n",
    "                                        prompt_prefix = self.prefix,\n",
    "                                        source = '',\n",
    "                                        prompt_suffix = '',\n",
    "                                        summary = ''\n",
    "                                       ).strip():\n",
    "                instr_prefix_idx = span_idx\n",
    "            \n",
    "            if tokenizer.decode(span_tokens) == self.prompt_template.format(instruction = self.instruction,\n",
    "                                        prompt_prefix = self.prefix,\n",
    "                                        source = source,\n",
    "                                        prompt_suffix = '',\n",
    "                                        summary = ''\n",
    "                                       ).strip():\n",
    "                instr_prefix_src_idx = span_idx\n",
    "            \n",
    "            if tokenizer.decode(span_tokens) == self.prompt_template.format(instruction = self.instruction,\n",
    "                                        prompt_prefix = self.prefix,\n",
    "                                        source = source,\n",
    "                                        prompt_suffix = self.suffix,\n",
    "                                        summary = ''\n",
    "                                       ).strip():\n",
    "                instr_prefix_src_suffix_idx = span_idx \n",
    "\n",
    "        return instr_idx, instr_prefix_idx, instr_prefix_src_idx, instr_prefix_src_suffix_idx\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "302e5307-3594-4124-9fee-ee1de6a14207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "prompt_template = f'{{instruction}}{{prompt_prefix}}{{source}}{{prompt_suffix}}{{summary}}'\n",
    "\n",
    "prompt_dict_path = '/home/ramprasad.sa/probing_summarization_factuality/datasets/prompt_templates/'\n",
    "prompt_type = 'document_context_causal'\n",
    "\n",
    "prompt_processor = PromptProcessor(prompt_template = prompt_template,\n",
    "                 prompt_dict_path = prompt_dict_path,\n",
    "                prompt_type = prompt_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1c61b36-6e34-4d88-813c-0913841f50bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "counter = 0\n",
    "\n",
    "for idx, row in df_genaudit_mistral[~df_genaudit_mistral['annotated_spans'].isnull()].iterrows():\n",
    "    source = row['source']\n",
    "    summary = row['summary']\n",
    "    nonfactual_spans = row['annotated_spans']\n",
    "    prompt = prompt_processor.make_prompt(source= source,\n",
    "                                 summary = summary)\n",
    "    if counter == 4:\n",
    "        break\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d3bc7c5-483e-47aa-be8b-462733d10c1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'peroxide<sep>earrings overnight<sep>mild'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(prompt)\n",
    "nonfactual_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "003907bc-8b4c-4074-8a2a-7cacc32d1c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Get non corrupted hidden states and save --> clean hstates\n",
    "Corrupt instruction embeddings --> save corrupted hidden states \n",
    "For all summary tokens as tgt\n",
    "    For all layers\n",
    "        For all clean tokens \n",
    "            Replace corrupted clean_token_layer with clean hstates \n",
    "            Get p(tgt_token) with corrupted \n",
    "            Get p(tgt_token) with corrupted and replaced \n",
    "            Accumulate difference --> {t1: {l1: , l2: ,,,}}\n",
    "'''\n",
    "import torch.nn.functional as F\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "from nnsight import LanguageModel, util\n",
    "from nnsight.tracing.Proxy import Proxy\n",
    "\n",
    "# util.apply(comparator, lambda x: x.value.item(), Proxy)\n",
    "\n",
    "\n",
    "source = source\n",
    "summary  = summary\n",
    "N_LAYERS = len(model.model.layers)\n",
    "\n",
    "prompt = prompt\n",
    "\n",
    "prompt_tokens = tokenizer(prompt).input_ids\n",
    "instr_idx, instr_prefix_idx, instr_prefix_src_idx, instr_prefix_src_suffix_idx = prompt_processor.get_prompt_attributes_idx(prompt_ids = tokenizer(prompt).input_ids,\n",
    "                              source = source,\n",
    "                              summary = summary,\n",
    "                              tokenizer = tokenizer)\n",
    "\n",
    "\n",
    "with model.trace() as tracer:\n",
    "    ''' clean run ''' \n",
    "    with tracer.invoke(prompt) as invoker:\n",
    "        clean_input_embeddings = model.model.embed_tokens.output.save()\n",
    "        \n",
    "        clean_hs = [\n",
    "            model.model.layers[layer_idx].output[0].save()\n",
    "            for layer_idx in range(N_LAYERS)\n",
    "        ]\n",
    "        clean_logits = model.lm_head.output[0]\n",
    "        clean_logits = F.softmax(clean_logits, dim = 1)\n",
    "\n",
    "    # ''' corrupted run '''\n",
    "    with tracer.invoke(prompt) as invoker:\n",
    "        init_noise = torch.zeros(clean_input_embeddings.shape)\n",
    "    #     ''' only noise the instruction tokens'''\n",
    "        init_noise[:, :instr_idx, :] = (0.1**0.5)*torch.randn(init_noise[:, :instr_idx, :].shape)\n",
    "        \n",
    "        model.model.embed_tokens.output = clean_input_embeddings + init_noise\n",
    "        corrupted_hs = [\n",
    "            model.model.layers[layer_idx].output[0].save()\n",
    "            for layer_idx in range(N_LAYERS)\n",
    "        ]\n",
    "        corrupted_logits = model.lm_head.output[0]\n",
    "        corrupted_logits = F.softmax(corrupted_logits, dim = 1).save()\n",
    "        noised_embeddings = model.model.embed_tokens.output.save() \n",
    "\n",
    "    comparator = []\n",
    "    # for token_idx in range(len(prompt_tokens)):\n",
    "    #     if token_idx >= instr_prefix_src_suffix_idx:\n",
    "    #         p_token_idx = clean_logits[token_idx - 1]\n",
    "    #         p_token_idx_corr \n",
    "    #         comparator.append(p_token[prompt_tokens[token_idx]].save())\n",
    "\n",
    "    for tidx in [470, 471, 475, 485, 486, 487, 492, 493, 494][:1]:\n",
    "\n",
    "        if tidx >= instr_prefix_src_suffix_idx:\n",
    "\n",
    "            prob_clean_tidx = clean_logits[tidx - 1]\n",
    "            prob_corr_tidx = corrupted_logits[tidx - 1]\n",
    "            pred_token = torch.argmax(prob_clean_tidx).item()\n",
    "            tgt_token = prompt_tokens[tidx]\n",
    "\n",
    "            layerwise_patching_results = []\n",
    "            for layer_idx in range(len(model.model.layers))[:1]:\n",
    "                # layerwise_patching_results = []\n",
    "                \n",
    "                with tracer.invoke(prompt) as invoker:\n",
    "                        model.model.embed_tokens.output = noised_embeddings\n",
    "                        model.model.layers[layer_idx].output[0].t[tidx - 1] = clean_hs[layer_idx].t[tidx - 1]\n",
    "                        corrupted_repl_logits = model.lm_head.output[0]\n",
    "                        corrupted_repl_logits = F.softmax(corrupted_repl_logits, dim = 1).save()\n",
    "                        \n",
    "                        # assert(corrupted_repl_logits\n",
    "                        prob_corr_repl_tidx = corrupted_repl_logits[tidx - 1]\n",
    "\n",
    "                        assert(corrupted_repl_logits[tidx - 1] == corrupted_logits[tidx - 1])\n",
    "                        layerwise_patching_results.append((layer_idx,\n",
    "                                                           tidx - 1,\n",
    "                                                           tgt_token, \n",
    "                                                           prob_clean_tidx[tgt_token].save(), \n",
    "                                                           prob_corr_tidx[tgt_token].save(),\n",
    "                                                           prob_corr_repl_tidx[tgt_token].save()))\n",
    "                        \n",
    "            \n",
    "                \n",
    "            comparator.append(layerwise_patching_results)\n",
    "\n",
    "        \n",
    "    \n",
    "                        \n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ca80a8f6-1f48-4ba4-831f-043a7d13b9f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True,  ..., True, True, True])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrupted_repl_logits[468] == corrupted_logits[468]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b7ce37a1-b1bc-4cb3-927b-85b5d7060f1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False,  ..., False, False, False])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrupted_repl_logits[469] == corrupted_logits[469]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6e0928a2-9ad9-4f6e-95ae-c610284c4251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0,\n",
       "   469,\n",
       "   8120,\n",
       "   tensor(0.8350, grad_fn=<SelectBackward0>),\n",
       "   tensor(0.9613, grad_fn=<SelectBackward0>),\n",
       "   tensor(0.9611, grad_fn=<SelectBackward0>))]]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "74d54473-257d-4ffd-92e6-6c92ac814278",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(clean_logits[instr_prefix_src_suffix_idx: ])\n",
    "\n",
    "pred_tokens = [torch.argmax(logit).item() for logit in clean_logits[instr_prefix_src_suffix_idx -1: -1]]\n",
    "summary_tokens = prompt_ids[instr_prefix_src_suffix_idx:]\n",
    "\n",
    "assert(len(pred_tokens) == len(summary_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "ec65dc80-6863-40a7-8b8b-8ca7bcc1e0ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instr_prefix_src_suffix_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "785813ef-24c5-40f9-84f6-d70af3859c39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(14.5398, grad_fn=<MaxBackward1>), 415)"
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(clean_logits[instr_prefix_src_suffix_idx -1 + 0]), summary_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "7daf691e-f8fa-4127-94ff-fd0e1d4e83d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "person document\n",
      "2\n",
      "a the\n",
      "4\n",
      "5\n",
      "6\n",
      "experience confusion\n",
      "8\n",
      "the a\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "frustration experience\n",
      "16\n",
      "trying spending\n",
      "18\n",
      "trying at\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "find open\n",
      "26\n",
      ". and\n",
      "realizing ultimately\n",
      "29\n",
      "30\n",
      "correct test\n",
      "was had\n",
      "33\n",
      "passed taken\n",
      "35\n",
      "on.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "counter = 0\n",
    "for pred, summ in list(zip(pred_tokens, summary_tokens)):\n",
    "    \n",
    "    if pred == summ:\n",
    "        print(counter)\n",
    "    else:\n",
    "        print(tokenizer.decode([pred, summ]))\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "d09d303d-df7c-4cfc-abaf-db6cb4597978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(415, 415),\n",
       " (1338, 3248),\n",
       " (13966, 13966),\n",
       " (264, 272),\n",
       " (3227, 3227),\n",
       " (28742, 28742),\n",
       " (28713, 28713),\n",
       " (2659, 16630),\n",
       " (684, 684),\n",
       " (272, 264),\n",
       " (1369, 1369),\n",
       " (3608, 3608),\n",
       " (304, 304),\n",
       " (652, 652),\n",
       " (14235, 14235),\n",
       " (22802, 2659),\n",
       " (302, 302),\n",
       " (2942, 9981),\n",
       " (727, 727),\n",
       " (2942, 438),\n",
       " (264, 264),\n",
       " (1486, 1486),\n",
       " (2052, 2052),\n",
       " (2942, 2942),\n",
       " (298, 298),\n",
       " (1300, 1565),\n",
       " (9289, 9289),\n",
       " (28723, 304),\n",
       " (27494, 12665),\n",
       " (27494, 27494),\n",
       " (272, 272),\n",
       " (4714, 1369),\n",
       " (403, 553),\n",
       " (2141, 2141),\n",
       " (4568, 3214),\n",
       " (1633, 1633),\n",
       " (356, 28723)]"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(pred_tokens, summary_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "59437fce-d3ba-40b3-af77-05b79b314e74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[415,\n",
       " 3248,\n",
       " 13966,\n",
       " 272,\n",
       " 3227,\n",
       " 28742,\n",
       " 28713,\n",
       " 16630,\n",
       " 684,\n",
       " 264,\n",
       " 1369,\n",
       " 3608,\n",
       " 304,\n",
       " 652,\n",
       " 14235,\n",
       " 2659,\n",
       " 302,\n",
       " 9981,\n",
       " 727,\n",
       " 438,\n",
       " 264,\n",
       " 1486,\n",
       " 2052,\n",
       " 2942,\n",
       " 298,\n",
       " 1565,\n",
       " 9289,\n",
       " 304,\n",
       " 12665,\n",
       " 27494,\n",
       " 272,\n",
       " 1369,\n",
       " 553,\n",
       " 2141,\n",
       " 3214,\n",
       " 1633,\n",
       " 28723]"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_ids = tokenizer(prompt).input_ids\n",
    "prompt_ids[instr_prefix_src_suffix_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "717ef64e-198c-4897-ba3a-125890c2d7e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0818, -0.1291, -0.0457,  ...,  0.0578, -0.0109,  0.0048]],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrupted_repl_hs[13][0][:, repl_token_idx, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "f80b19a2-2e6c-41bd-bf88-79419be76cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 True\n",
      "1 True\n",
      "2 True\n",
      "3 True\n",
      "4 True\n",
      "5 True\n",
      "6 True\n",
      "7 True\n",
      "8 True\n",
      "9 False\n",
      "10 False\n",
      "11 False\n",
      "12 False\n",
      "13 False\n",
      "14 False\n",
      "15 False\n",
      "16 False\n",
      "17 False\n",
      "18 False\n",
      "19 False\n",
      "20 False\n",
      "21 False\n",
      "22 False\n",
      "23 False\n",
      "24 False\n",
      "25 False\n",
      "26 False\n",
      "27 False\n",
      "28 False\n",
      "29 False\n",
      "30 False\n",
      "31 False\n"
     ]
    }
   ],
   "source": [
    "# corrupted_hs[0] == corrupted_repl_hs[0]\n",
    "for layer_idx in range(0, 32):\n",
    "    if layer_idx == 9:\n",
    "        differing = corrupted_hs[layer_idx] == corrupted_repl_hs[layer_idx]\n",
    "        # print(corrupted_hs[layer_idx] == corrupted_repl_hs[layer_idx])\n",
    "    print(layer_idx, torch.allclose(corrupted_hs[layer_idx], corrupted_repl_hs[layer_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "46c57ab0-9c73-4b2f-89d6-4b06aa5f04fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0,    4,    0],\n",
       "        [   0,    4,    1],\n",
       "        [   0,    4,    2],\n",
       "        ...,\n",
       "        [   0,    4, 4093],\n",
       "        [   0,    4, 4094],\n",
       "        [   0,    4, 4095]])"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import torch\n",
    "\n",
    "# # Example boolean tensor\n",
    "# boolean_tensor = torch.tensor([[True, False, True], [False, True, False]])\n",
    "\n",
    "# # Find indices of False values\n",
    "# false_indices = (boolean_tensor == False).nonzero()\n",
    "\n",
    "# print(\"Indices of False values:\")\n",
    "# print(false_indices)\n",
    "\n",
    "false_indices = (differing == False).nonzero()\n",
    "false_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "9f3cd935-9f41-467f-b34e-a9c077239adc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 137, 4096])"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "differing.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "f800db27-55d3-4c4e-a835-30e3f3d4e9d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Save original output\n",
    "Corrupt input embeddings \n",
    "Save corrupted output \n",
    "\n",
    "For each layer and token replace corrupted with original\n",
    "'''\n",
    "import torch\n",
    "\n",
    "N_LAYERS = 32\n",
    "with model.trace() as tracer:\n",
    "    #### clean run ####\n",
    "    with tracer.invoke(prompt) as invoker:\n",
    "        clean_tokens = model.input[1][\"input_ids\"].squeeze().save()\n",
    "        embeddings = model.model.embed_tokens.output.save()\n",
    "        clean_hs = [\n",
    "            model.model.layers[layer_idx].output[0].save()\n",
    "            for layer_idx in range(N_LAYERS)\n",
    "        ]\n",
    "        clean_logits = model.lm_head.output.save()\n",
    "        \n",
    "    #### corrupted run #### \n",
    "    with tracer.invoke(prompt): \n",
    "        # Noise the input embeddings for instructions only \n",
    "        noise = torch.zeros(embeddings.shape)\n",
    "        noise[:, :prompt_instruction_ids.shape[-1], :] = (0.1**0.5)*torch.randn(noise[:, :prompt_instruction_ids.shape[-1], :].shape)\n",
    "        \n",
    "        corrupted_logits = model.lm_head.output.save()\n",
    "\n",
    "        \n",
    "        \n",
    "        ''' Test output differences '''\n",
    "        # model.model.embed_tokens.output = embeddings + noise\n",
    "        \n",
    "        # corrupted_output = [\n",
    "        #     model.model.layers[layer_idx].output[0].save()\n",
    "        #     for layer_idx in range(N_LAYERS)\n",
    "        # ]\n",
    "        # corrupted_logits = model.lm_head.output.save()\n",
    "\n",
    "    # #### corrupted with replacement \n",
    "    # for tgt_token_idx in range(1, len(clean_tokens)):\n",
    "    #     tgt_token = clean_tokens[tgt_token_idx]\n",
    "    #     p_tgt_token_\n",
    "    #     p_corrupted_tgt_token = \n",
    "\n",
    "        \n",
    "    #     for layer_idx in range(len(model.model.layers)):\n",
    "    #         for token_idx in range(len(clean_tokens)):\n",
    "                \n",
    "            \n",
    "    # for token_idx in range(len(clean_tokens)):\n",
    "    #     for layer_idx in range(len(model.model.layers)):\n",
    "        \n",
    "    #         #### Making sure the predicted token is not beyond our summary/prompt\n",
    "    #         tgt_idx = token_idx + 1 \n",
    "    #         if tgt_idx < len(clean_tokens):\n",
    "    #             tgt_token = clean_tokens[tgt_idx]\n",
    "\n",
    "    #             model.model.layers[layer_idx].output[0].t[token_idx] = \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8bfeed-b2bd-4525-a8e3-93398745dd36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "e2cc4f51-ee44-4867-9ee5-16a5a97cf60a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    1, 26075,   264, 14060,   354,   272,  2296,  3248,   297,  6817,\n",
       "        28723,  1684,  6818,   272, 14060, 28725,   865,   938,  1871,   369,\n",
       "          349,  2169,   297,   272,  3248,  4192, 28738,  2431, 28747,   613,\n",
       "         1654,   586,   960,  1369,   403,  3154,   568,   378,   403, 12091,\n",
       "          568,   613,  5223,   264,  3102,   302, 28705, 28770, 28734,  3486,\n",
       "          442,   579,  7312,  1401,   264,  1486, 19185,  2942,   298,  1565,\n",
       "         1012,  2692,  2251,   568,   613,  4251,  9681,   586,  1369, 13490,\n",
       "          304,  7185,   272,  3608,   773,  2495,   324,  1466,   461,  1802,\n",
       "        28705, 28740, 28770,   568,   708,  3383,   708,   624,  1112,   403,\n",
       "          297,   272, 12128,  2055, 28723,   318,  4171, 28755, 10713, 28747,\n",
       "          415,  3248, 13966,   272,  3227, 28742, 28713, 16630,   684,   264,\n",
       "         1369,  3608,   304,   652, 14235,  2659,   302,  9981,   727,   438,\n",
       "          264,  1486,  2052,  2942,   298,  1565,  9289,   304, 12665, 27494,\n",
       "          272,  1369,   553,  2141,  3214,  1633, 28723])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "b6e208ad-72e1-4102-81be-c4792e4b4418",
   "metadata": {},
   "outputs": [],
   "source": [
    "instr_idx, instr_prefix_idx, instr_prefix_src_idx, instr_prefix_src_suffix_idx = prompt_processor.get_prompt_attributes_idx(prompt_ids = clean_tokens,\n",
    "                          source = source,\n",
    "                          summary = summary,\n",
    "                          tokenizer = tokenizer)\n",
    "\n",
    "summary_tokens = clean_tokens[instr_prefix_src_suffix_idx:]\n",
    "assert(tokenizer.decode(summary_tokens) == summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "2228860d-91b9-4683-81cf-d068321be42a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The document describes the author's confusion about a test date and their subsequent experience of spending time at a high school trying to open doors and ultimately realizing the test had already taken place.\""
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "274f4b7e-24de-48d6-b5a8-bd0aec4b3d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_repl = 13\n",
    "token_repl = 26\n",
    "\n",
    "with model.trace() as tracer:\n",
    "    with tracer.invoke(prompt): \n",
    "        model.model.embed_tokens.output = embeddings + noise\n",
    "        model.model.layers[layer_repl].output[0].t[token_repl] = clean_hs[layer_repl].t[token_repl]\n",
    "        corrupted_replaced_logits = model.lm_head.output.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "fcc65f7b-1110-4239-8d78-10692f9be3c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'source_len' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[91], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mdecode(clean_tokens), \u001b[43msource_len\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'source_len' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer.decode(clean_tokens), source_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2e427282-27d3-44c2-b43b-e732b35b4de4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"i test is a protagon's experience and the test date and their subsequent realization of walking time trying a high school trying to find doors. finding realizing the test date already passed place. The\""
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrupted_summ_logits = corrupted_replaced_logits[:,-summ_len - 1:,:]\n",
    "\n",
    "# clean_summary_logits.squeeze(0).shape\n",
    "tokens = [torch.argmax(each).item() for each in corrupted_summ_logits.squeeze(0)]\n",
    "tokenizer.decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "dcd55fa9-5863-4966-8e42-6839ee91bb95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"i test is the protagon's experience and the test date and their subsequent realization of walking time trying a high school trying to find doors. finding realizing the test date already passed place. The\""
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrupted_summ_logits = corrupted_logits[:,-summ_len - 1:,:]\n",
    "\n",
    "# clean_summary_logits.squeeze(0).shape\n",
    "tokens = [torch.argmax(each).item() for each in corrupted_summ_logits.squeeze(0)]\n",
    "tokenizer.decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "11911e9b-b681-4bca-98b2-c51024caf607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The author describes the author's confusion about the test date and their subsequent attempt of trying time trying a high school trying to find doors until realizing realizing the error was already passed place on</s>\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_summary_logits = clean_logits[:,-summ_len - 1:,:]\n",
    "\n",
    "# clean_summary_logits.squeeze(0).shape\n",
    "tokens = [torch.argmax(each).item() for each in clean_summary_logits.squeeze(0)]\n",
    "tokenizer.decode(tokens)\n",
    "# tokens = [torch.argmax(clean_logits[:, token_idx, :]).item() for token_idx in range(clean_logits.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "46bc2587-683c-4f62-890a-667389601d7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The document describes the author's confusion about a test date and their subsequent experience of spending time at a high school trying to open doors and ultimately realizing the test had already taken place.\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5780b5dd-a28f-4cb7-8264-5ab8d5c85e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(prompt,\n",
    "          return_tensors=\"pt\").input_ids\n",
    "mistral_model_output = mistral_model.generate(input_ids,\n",
    "                      max_length=input_ids.shape[-1] + 1, \n",
    "                        output_attentions = True,\n",
    "                        output_hidden_states=True, \n",
    "                        return_dict_in_generate=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9468fcc7-bb0b-48a8-9f15-336e18b17f12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summ_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ffa904fe-9733-4606-b12f-02a5f2b621f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "hstates = torch.cat(mistral_model_output['hidden_states'][0])\n",
    "summary_hstates = hstates[:, -summ_len-1:, :]\n",
    "logits = [mistral_model.lm_head(hstate) for hstate in summary_hstates[-1]]\n",
    "# hstates[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e28b15a5-1af0-43da-865b-e4877b6ee0da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The author describes the author's confusion about the test date and their subsequent attempt of trying time trying a high school trying to find doors until realizing realizing the error was already passed place on</s>\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = [torch.argmax(logit).item() for logit in logits]\n",
    "tokenizer.decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "298ef995-3c12-4892-b0ab-71d9fa14388c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The document describes the author's confusion about a test date and their subsequent experience of spending time at a high school trying to open doors and ultimately realizing the test had already taken place.\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "eb27275f-bb1b-4b23-91ad-5e1dc5678fc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Q a list of the following story: .\\n the a summary, consider include the that is relevant in the document and\\n: https the and my friend are in a sports\\n the local school. we am and she playsences\\n\\n we the great start, both andi:)) and i was cold cold that have follow in in the formal, i girlfriend was a black meet today and so she practices out at 3:30pm she practice is at 3:05..\\n is to dress home and she can for for locker room for her to she can help her my ride beforebye before she do did't know is that she girlfriend is at she amm waiting outside the locker rooms waiting i late senior walkresser, takes not no for  bus minute.. i i gets out she give her a kiss goodbye and i to.. i is a wrong no there nothing me bus soundfall.. it i wased and start walking my to i girlfriend who i' a off at and i call my mom and my one.. i mom are i brother are out late i one.. them either i i'm freak  bus for my 20 walk home in thezing weather.. the swim swim and and jeans jacket.. hat.. i i luck on\\n\\n[\\n: The author describes a author of the high school student who participates in winter sports. his girlfriend. The was the student had a late meet and must dress up for it. After, their girlfriend has a normal practice and ends at 3:15.. The student waits outside the locker rooms for give their girlfriend a kissbye kiss, they leave for their practice practices.. However, the student'izes that their bus leaves while their are waiting, they are it.. As a result, the student has to walk  miles in freezing temperatures to get home. wearing only a dress shirt and no gloves or hat.</s> document ends with the student wishing for luck.</s>\""
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "edc7a988-d2d4-4966-a607-fc8bf34d21fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The document describes the experience of a high school student who participates in winter sports with their girlfriend.. Today, the student has a late meet and must dress up for it.. Meanwhile, their girlfriend has a normal practice that ends at 3:15 PM.. The student waits outside the locker rooms to give their girlfriend a goodbye kiss before they leave for their respective activities.. However, the student realizes that their bus leaves while they are waiting and they miss it.. As a result, the student has to walk six miles in freezing temperatures to get home, wearing only a dress shirt and no gloves or hat.. The document ends with the student wishing for luck.'"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "32b8a236-4b57-401c-af51-08a74b74e99e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32000])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_logits[:, 0, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c498eb-e91c-4acb-986f-b97bc9e93020",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nnsight)",
   "language": "python",
   "name": "nnsight"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
