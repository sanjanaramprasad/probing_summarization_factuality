{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b8a01b0b-759b-4c9b-bdfb-51f9cdd2fcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "from nnsight import LanguageModel, util\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from nnsight.tracing.Proxy import Proxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "07a38b0e-3eb5-4eec-adf8-fefb3312e7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_path = {'mistral7b': 'mistralai/Mistral-7B-Instruct-v0.1',\n",
    "             'falcon7b': 'tiiuae/falcon-7b-instruct',\n",
    "             'llama7b': '/work/frink/models/Llama-2-7b-chat-hf',\n",
    "             'flanul2': 'google/flan-ul2'}\n",
    "\n",
    "def load_tokenizer(model_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path[model_name],\n",
    "                                         cache_dir = '/scratch/ramprasad.sa/huggingface_models')\n",
    "\n",
    "    \n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0133bde3-6a90-4535-a293-099fa87c786c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ramprasad.sa/.conda/envs/nnsight/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = load_tokenizer('mistral7b')\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc6c3646-724c-4045-a833-2840937487d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>source</th>\n",
       "      <th>summary</th>\n",
       "      <th>annotated_spans</th>\n",
       "      <th>model</th>\n",
       "      <th>origin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Rachel Usher#REDDIT-83:mistral7b-ul2</td>\n",
       "      <td>both me and my girlfriend participate in winte...</td>\n",
       "      <td>The document describes the experience of a hig...</td>\n",
       "      <td>participates&lt;sep&gt;with their girlfriend&lt;sep&gt;ends</td>\n",
       "      <td>mistral7b</td>\n",
       "      <td>REDDIT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                    id  \\\n",
       "4           4  Rachel Usher#REDDIT-83:mistral7b-ul2   \n",
       "\n",
       "                                              source  \\\n",
       "4  both me and my girlfriend participate in winte...   \n",
       "\n",
       "                                             summary  \\\n",
       "4  The document describes the experience of a hig...   \n",
       "\n",
       "                                   annotated_spans      model  origin  \n",
       "4  participates<sep>with their girlfriend<sep>ends  mistral7b  REDDIT  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'mistral7b'\n",
    "genaudit_read_path = '/home/ramprasad.sa/probing_summarization_factuality/datasets/Genaudit_annotations.csv'\n",
    "df_genaudit = pd.read_csv(genaudit_read_path)\n",
    "df_genaudit_mistral = df_genaudit[df_genaudit['model'] == model_name]\n",
    "df_genaudit_mistral.head()[:1]\n",
    "# df_genaudit_mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2df8e57c-42e4-4e7e-9f19-6fa014c2455a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'instruction': 'Generate a summary for the following document in brief. When creating the summary, only use information that is present in the document',\n",
       "  'prompt_prefix_template': ' CONTENT: ',\n",
       "  'prompt_suffix_template': ' SUMMARY: '},\n",
       " '{instruction}{prompt_prefix}{source}{prompt_suffix}{summary}')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json \n",
    "prompt_template = f'{{instruction}}{{prompt_prefix}}{{source}}{{prompt_suffix}}{{summary}}'\n",
    "\n",
    "prompt_template_path = '/home/ramprasad.sa/probing_summarization_factuality/datasets/prompt_templates/'\n",
    "prompt_type = 'document_context_causal'\n",
    "\n",
    "with open(f'{prompt_template_path}/{prompt_type}.json', 'r') as fp:\n",
    "    prompt_dict = json.load(fp)\n",
    "prompt_dict, prompt_template\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2441cee-b1b3-4ecf-a26b-bee2b2bba6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def strip_bos_eos_ids(ids):\n",
    "    ids = ids[:, 1:] if ids[0][0] in [0,1,2] else ids\n",
    "    return ids\n",
    "    \n",
    "class PromptProcessor():\n",
    "\n",
    "    def __init__(self,\n",
    "                 prompt_template,\n",
    "                 prompt_template_path,\n",
    "                prompt_type,\n",
    "                tokenizer):\n",
    "\n",
    "        self.prompt_template = prompt_template\n",
    "\n",
    "        with open(f'{prompt_template_path}/{prompt_type}.json', 'r') as fp:\n",
    "            self.prompt_dict = json.load(fp)\n",
    "            \n",
    "        self.instruction = self.prompt_dict['instruction'] if 'instruction' in self.prompt_dict else ''\n",
    "        self.prefix = self.prompt_dict['prompt_prefix_template'] if 'prompt_prefix_template' in self.prompt_dict else ''\n",
    "        self.suffix = self.prompt_dict['prompt_suffix_template'] if 'prompt_suffix_template' in self.prompt_dict else ''\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def get_prompt_attributes_idx(self,\n",
    "                                  prompt_ids,\n",
    "                                  source,\n",
    "                                  summary):\n",
    "        instr_idx = -1\n",
    "        instr_prefix_idx = -1\n",
    "        instr_prefix_src_idx = -1\n",
    "        instr_prefix_src_suffix_idx = -1\n",
    "        print('attri', len(prompt_ids))\n",
    "        for span_idx in range( len(prompt_ids)):\n",
    "            if span_idx %100 == 0:\n",
    "                print(span_idx)\n",
    "            span_tokens = prompt_ids[1:span_idx]\n",
    "            if self.tokenizer.decode(span_tokens) == self.prompt_template.format(instruction = self.instruction,\n",
    "                                            prompt_prefix = '',\n",
    "                                            source = '',\n",
    "                                            prompt_suffix = '',\n",
    "                                            summary = ''\n",
    "                                           ).strip():\n",
    "                instr_idx = span_idx\n",
    "            \n",
    "            if self.tokenizer.decode(span_tokens) == self.prompt_template.format(instruction = self.instruction,\n",
    "                                        prompt_prefix = self.prefix,\n",
    "                                        source = '',\n",
    "                                        prompt_suffix = '',\n",
    "                                        summary = ''\n",
    "                                       ).strip():\n",
    "                instr_prefix_idx = span_idx\n",
    "            \n",
    "            if self.tokenizer.decode(span_tokens) == self.prompt_template.format(instruction = self.instruction,\n",
    "                                        prompt_prefix = self.prefix,\n",
    "                                        source = source,\n",
    "                                        prompt_suffix = '',\n",
    "                                        summary = ''\n",
    "                                       ).strip():\n",
    "                instr_prefix_src_idx = span_idx\n",
    "            \n",
    "            if self.tokenizer.decode(span_tokens) == self.prompt_template.format(instruction = self.instruction,\n",
    "                                        prompt_prefix = self.prefix,\n",
    "                                        source = source,\n",
    "                                        prompt_suffix = self.suffix,\n",
    "                                        summary = ''\n",
    "                                       ).strip():\n",
    "                instr_prefix_src_suffix_idx = span_idx \n",
    "                \n",
    "        return instr_idx, instr_prefix_idx, instr_prefix_src_idx, instr_prefix_src_suffix_idx\n",
    "    \n",
    "    def get_nonfactual_span_idx(self,\n",
    "                                nonfactual_span,\n",
    "                                summary_tokens,\n",
    "                                start_idx = -100,\n",
    "                                end_idx = -100):\n",
    "        # print(len(summary_tokens), end_idx, nonfactual_span)\n",
    "        for tok_idx, tok in enumerate(summary_tokens):\n",
    "            if tok_idx > end_idx:\n",
    "                tok_str = self.tokenizer.decode(tok)\n",
    "                if tok_str in nonfactual_span[:len(tok_str)]:\n",
    "                    start_idx = tok_idx\n",
    "                \n",
    "                if self.tokenizer.decode(summary_tokens[start_idx: tok_idx + 1]) == nonfactual_span:\n",
    "                    end_idx = tok_idx\n",
    "                    break\n",
    "        \n",
    "        return start_idx, end_idx\n",
    "\n",
    "    def get_summary_labels(self,\n",
    "                          summary_tokens,\n",
    "                          nonfactual_spans):\n",
    "        start_idx = -100\n",
    "        end_idx = -100\n",
    "        summary_labels = [0] * len(summary_tokens)\n",
    "        nonfactual_spans = nonfactual_spans.split('<sep>')\n",
    "\n",
    "        for i in range(len(nonfactual_spans)):\n",
    "            nonfactual_span = nonfactual_spans.pop(0)\n",
    "            start_idx, end_idx = self.get_nonfactual_span_idx(nonfactual_span,\n",
    "                                             summary_tokens,\n",
    "                                             start_idx = start_idx,\n",
    "                                             end_idx = end_idx)\n",
    "    \n",
    "            for idx in range(start_idx, end_idx + 1):\n",
    "                summary_labels[idx] = 1\n",
    "        return summary_labels\n",
    "        \n",
    "    \n",
    "    def make_prompt_token_labels(self,\n",
    "                                 source,\n",
    "                                 summary,\n",
    "                                 nonfactual_spans):\n",
    "        source = ' '.join(source.split(' ')[:50])\n",
    "        prompt = self.prompt_template.format(instruction = self.instruction,\n",
    "                                    prompt_prefix = self.prefix,\n",
    "                                    source = source,\n",
    "                                    prompt_suffix = self.suffix,\n",
    "                                    summary = summary\n",
    "                                   )\n",
    "\n",
    "        \n",
    "        prompt_tokens = tokenizer(prompt).input_ids\n",
    "        instr_idx, instr_prefix_idx, instr_prefix_src_idx, instr_prefix_src_suffix_idx = self.get_prompt_attributes_idx(prompt_ids = prompt_tokens,\n",
    "                                      source = source,\n",
    "                                      summary = summary)\n",
    "        \n",
    "        summary_tokens = prompt_tokens[instr_prefix_src_suffix_idx:]\n",
    "        \n",
    "        summary_labels = self.get_summary_labels(summary_tokens,\n",
    "                          nonfactual_spans)\n",
    "\n",
    "        return_dict = {'prompt': prompt,\n",
    "                      'instr_idx': instr_idx,\n",
    "                      'instr_prefix_idx': instr_prefix_idx,\n",
    "                      'instr_prefix_src_idx': instr_prefix_src_idx,\n",
    "                      'instr_prefix_src_suffix_idx': instr_prefix_src_suffix_idx,\n",
    "                      'summary_labels': summary_labels}\n",
    "        return return_dict\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95b811e8-14ba-4f2e-91c0-01a6a5b17e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_processor = PromptProcessor(prompt_template = prompt_template,\n",
    "                                   prompt_template_path = prompt_template_path,\n",
    "                                   prompt_type = prompt_type,\n",
    "                                   tokenizer = tokenizer\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55a4f4d7-68e3-4175-ae95-bd2f503c729a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attri 231\n",
      "0\n",
      "100\n",
      "200\n",
      "1\n",
      "attri 258\n",
      "0\n",
      "100\n",
      "200\n",
      "2\n",
      "attri 214\n",
      "0\n",
      "100\n",
      "200\n",
      "3\n",
      "attri 250\n",
      "0\n",
      "100\n",
      "200\n",
      "4\n",
      "attri 165\n",
      "0\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "\n",
    "for idx, row in df_genaudit_mistral[~df_genaudit_mistral['annotated_spans'].isnull()].iterrows():\n",
    "    source = row['source']\n",
    "    summary = row['summary']\n",
    "    nonfactual_spans = row['annotated_spans']\n",
    "    prompt_dict = prompt_processor.make_prompt_token_labels(source= source,\n",
    "                                                            summary = summary,\n",
    "                                                            nonfactual_spans = nonfactual_spans)\n",
    "    if counter == 4:\n",
    "        break\n",
    "    counter += 1\n",
    "    print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1389933d-9a42-47a0-b57f-a9fe9dc013d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['prompt', 'instr_idx', 'instr_prefix_idx', 'instr_prefix_src_idx', 'instr_prefix_src_suffix_idx', 'summary_labels'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_dict.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23bd0127-8609-4470-a441-ac673de316bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = {'mistral7b': 'mistralai/Mistral-7B-Instruct-v0.1',\n",
    "             'falcon7b': 'tiiuae/falcon-7b-instruct',\n",
    "             'llama7b': '/work/frink/models/Llama-2-7b-chat-hf',\n",
    "             'flanul2': 'google/flan-ul2'}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_model(model_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path[model_name],\n",
    "                                         cache_dir = '/scratch/ramprasad.sa/huggingface_models')\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path[model_name],\n",
    "                                            cache_dir = '/scratch/ramprasad.sa/huggingface_models')\n",
    "    model = model.to('cuda')\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24766189-1e89-4d55-89a3-fbe3447de7d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ramprasad.sa/.conda/envs/nnsight/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bab7e279036940dfa819adfe4b617719",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MistralRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm()\n",
       "        (post_attention_layernorm): MistralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "  (generator): WrapperModule()\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer, mistral_model = load_model(model_name)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "model = LanguageModel(mistral_model, tokenizer=tokenizer, device_map=\"auto\", dispatch=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9dd1139c-736f-430c-966e-6e03598c3db6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_LAYERS = len(model.model.layers)\n",
    "N_LAYERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5149531e-76c2-427a-8acb-943115a7cb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "source = source\n",
    "summary  = summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9853da7-5c94-47dd-bd19-5f7213c42a49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = prompt_dict['prompt']\n",
    "instr_idx = prompt_dict['instr_idx']\n",
    "instr_prefix_idx = prompt_dict['instr_prefix_idx']\n",
    "instr_prefix_src_idx = prompt_dict['instr_prefix_src_idx']\n",
    "instr_prefix_src_suffix_idx = prompt_dict['instr_prefix_src_suffix_idx']\n",
    "instr_prefix_src_suffix_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d8ef587-1523-4966-9b27-612bba083744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "90d92454-02f7-43b8-83e7-ee01199d4be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean_logits(prompt,\n",
    "                    model):\n",
    "    with model.trace() as tracer:\n",
    "        with tracer.invoke(prompt) as invoker:\n",
    "            \n",
    "            clean_logits = model.lm_head.output[0]\n",
    "            clean_logits = F.softmax(clean_logits, dim = 1).detach().cpu().save()\n",
    "            \n",
    "            clean_hs = [\n",
    "                model.model.layers[layer_idx].output[0].detach().cpu().save()\n",
    "                for layer_idx in range(N_LAYERS)\n",
    "            ]\n",
    "            \n",
    "            clean_input_embeddings = model.model.embed_tokens.output.detach().cpu().save()\n",
    "            \n",
    "    return clean_input_embeddings, clean_hs, clean_logits\n",
    "\n",
    "def get_corrupted_logits(corrupted_prompt,\n",
    "                        model):\n",
    "\n",
    "\n",
    "\n",
    "    with model.trace() as tracer:\n",
    "        with tracer.invoke(corrupted_prompt) as invoker:\n",
    "            \n",
    "            corrupted_logits = model.lm_head.output[0]\n",
    "            corrupted_logits = F.softmax(corrupted_logits, dim = 1).detach().cpu().save()\n",
    "            \n",
    "            corrupted_hs = [\n",
    "                model.model.layers[layer_idx].output[0].detach().cpu().save()\n",
    "                for layer_idx in range(N_LAYERS)\n",
    "            ]\n",
    "            \n",
    "            noised_embeddings = model.model.embed_tokens.output.detach().cpu().save()\n",
    "    return noised_embeddings, corrupted_hs, corrupted_logits\n",
    "\n",
    "\n",
    "def get_patched_logits(corrupted_prompt,\n",
    "                      layer_idx,\n",
    "                      token_idx):\n",
    "    with model.trace() as tracer:\n",
    "        with tracer.invoke(corrupted_prompt) as invoker:\n",
    "            model.model.layers[layer_idx].output[0].t[token_idx] = clean_hs[layer_idx].t[token_idx]\n",
    "            \n",
    "            patched_logits = model.lm_head.output[0]\n",
    "            patched_logits = F.softmax(patched_logits, dim = 1).detach().cpu().save()\n",
    "            patched_hs = [\n",
    "                model.model.layers[layer_idx].output[0].detach().cpu().save()\n",
    "                for layer_idx in range(N_LAYERS)\n",
    "            ]\n",
    "            \n",
    "            patched_embeddings = model.model.embed_tokens.output.detach().cpu().save()\n",
    "            \n",
    "    return patched_embeddings, patched_hs, patched_logits\n",
    "\n",
    "\n",
    "def corrupt_prompt(prompt_tokens,\n",
    "                   corruption_idx):\n",
    "    repl_token = tokenizer('_').input_ids[1:]\n",
    "    assert(len(repl_token) == 1)\n",
    "    repl_token = repl_token[0]\n",
    "\n",
    "    corrupted_tokens = [repl_token if tok_idx in corruption_idx else tok.item() for tok_idx, tok in enumerate(prompt_tokens)] \n",
    "    corrupted_prompt = tokenizer.decode(corrupted_tokens)\n",
    "    return corrupted_prompt\n",
    "\n",
    "\n",
    "with model.trace() as tracer:\n",
    "    with tracer.invoke(prompt) as invoker:\n",
    "        clean_tokens = model.input[1][\"input_ids\"].squeeze().save()\n",
    "\n",
    "clean_input_embeddings, clean_hs, clean_logits = get_clean_logits(prompt,\n",
    "                    model)\n",
    "\n",
    "corruption_idx = [i for i in range(prompt_dict['instr_idx'])]\n",
    "corrupted_prompt = corrupt_prompt(prompt_tokens = clean_tokens,\n",
    "               corruption_idx = corruption_idx)\n",
    "noised_embeddings, corrupted_hs, corrupted_logits = get_corrupted_logits(corrupted_prompt,\n",
    "                                                                             model = model)\n",
    "\n",
    "##### corruption with replacement for summary tokens\n",
    "\n",
    "# summary_tokens = clean_tokens[instr_prefix_src_suffix_idx:]\n",
    "# summary_label = prompt_dict['summary_labels']\n",
    "# assert(summary_tokens.shape[-1] == len(summary_label))\n",
    "# summary_token_patching_results = []\n",
    "# summ_idx = 0\n",
    "# for tidx, tgt_token in enumerate(clean_tokens):\n",
    "#         if tidx >= instr_prefix_src_suffix_idx:\n",
    "#             assert(tgt_token == summary_tokens[summ_idx])\n",
    "            \n",
    "\n",
    "#             layer_wise_patching_results = []\n",
    "# #             with model.trace() as tracer:\n",
    "#             for layer_idx in range(len(model.model.layers)):\n",
    "                    \n",
    "#                     _, patched_hs, patched_logits =  get_patched_logits(prompt,\n",
    "#                       noised_embeddings,\n",
    "#                       layer_idx,\n",
    "#                       token_idx = tidx - 1)\n",
    "\n",
    "#                     append_dict = {\n",
    "#                         'layer': layer_idx,\n",
    "#                         'target': tgt_token.item(),\n",
    "#                         'predicted': torch.argmax(clean_logits[tidx - 1]).item(),\n",
    "#                         'factual_label': summary_label[summ_idx],\n",
    "#                         'prob_clean': clean_logits[tidx - 1][tgt_token].item(),\n",
    "#                         'prob_corrupted': corrupted_logits[tidx - 1][tgt_token].item(),\n",
    "#                         'prob_patched': patched_logits[tidx - 1][tgt_token].item()\n",
    "#                     }\n",
    "#                     layer_wise_patching_results.append(append_dict)\n",
    "#                     break\n",
    "                \n",
    "\n",
    "#             summary_token_patching_results.append(layer_wise_patching_results)\n",
    "#             summ_idx += 1\n",
    "#             if summ_idx == 10:\n",
    "#                 break\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d8f85832-7755-41e0-b8ff-523a7f9278b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'c' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m corrupted_hs, \u001b[43mc\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'c' is not defined"
     ]
    }
   ],
   "source": [
    "corrupted_hs, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936a47a1-5a8c-481d-9810-a0c1d8645661",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(summary_token_patching_results)\n",
    "import re\n",
    "import string\n",
    "\n",
    "def check_only_trailing_punctuations(token_list):\n",
    "    regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    for idx in [0, len(token_list) - 1]:\n",
    "        if idx <= len(token_list) - 1:\n",
    "            if not regex.sub('', token_list[idx][1]).strip():\n",
    "                token_list.pop(idx)\n",
    "    return token_list\n",
    "\n",
    "def add_word_list(token_list, word_list):\n",
    "    token_list = check_only_trailing_punctuations(token_list)\n",
    "    if token_list:\n",
    "        word_list.append(token_list)\n",
    "    return word_list\n",
    "    \n",
    "def get_word_based_idx(summary_tokens):\n",
    "    all_words = []\n",
    "    token_list = []\n",
    "    for summ_tok_idx, summ_tok in enumerate(summary_tokens):\n",
    "        prefix_tok_str = ''\n",
    "        current_tok_str = tokenizer.decode(summ_tok)\n",
    "        if summ_tok_idx > 0:\n",
    "            prefix_tok_str = tokenizer.decode([summary_tokens[summ_tok_idx - 1], summ_tok])\n",
    "\n",
    "        #### new token is start of a new word \n",
    "        if len(prefix_tok_str.split(' '))> 1:\n",
    "\n",
    "            ### append stored tokens of a previous word\n",
    "            if len(token_list) > 0:\n",
    "                all_words = add_word_list(token_list, all_words)\n",
    "\n",
    "            #### now reset token list for new word \n",
    "            token_list = [(summ_tok_idx, current_tok_str, summ_tok.item(), )]\n",
    "            \n",
    "        else:\n",
    "            token_list.append((summ_tok_idx, current_tok_str, summ_tok.item()))\n",
    "\n",
    "    all_words = add_word_list(token_list, all_words)\n",
    "    return all_words\n",
    "\n",
    "\n",
    "\n",
    "    # print(current_tok, len(word_list))\n",
    "    # if len(prefix_str.split(' ')) \n",
    "    # if prev_tok is None or \n",
    "    # summ_tok_label = summary_label[summ_tok_idx]\n",
    "    # summ_tok_layers_patches = summary_token_patching_results[summ_tok_idx]\n",
    "    # for layer_idx, tgt_token, _, pred_token, p_clean, p_corr, p_patched in summ_tok_layers_patches:\n",
    "    #     ie_score = (p_patched - p_corr) / (p_clean - p_corr)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7f1d042c-8977-48ea-b69c-a2572b98fb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_word_list = get_word_based_idx(summary_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c10cc80b-7d52-4457-bc71-05c7a80c744b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 'The', 415)],\n",
       " [(1, 'document', 3248)],\n",
       " [(2, 'describes', 13966)],\n",
       " [(3, 'a', 264)],\n",
       " [(4, 'situation', 4620)]]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_word_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e8012fcf-70ac-4480-806e-6af43e6d4a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 'The', 415)]\n",
      "[(1, 'document', 3248)]\n",
      "[(2, 'describes', 13966)]\n",
      "[(3, 'a', 264)]\n",
      "[(4, 'situation', 4620)]\n",
      "[(5, 'where', 970)]\n",
      "[(6, 'a', 264)]\n",
      "[(7, 'person', 1338)]\n",
      "[(8, 'switched', 21187)]\n",
      "[(9, 'out', 575)]\n",
      "[(10, 'their', 652)]\n",
      "[(11, 'ear', 8120), (12, 'rings', 26661)]\n",
      "[(13, 'and', 304)]\n",
      "[(14, 'tried', 3851)]\n",
      "[(15, 'to', 298)]\n",
      "[(16, 'clean', 3587)]\n",
      "[(17, 'them', 706)]\n",
      "[(18, 'using', 1413)]\n",
      "[(19, 'per', 660), (20, 'ox', 1142), (21, 'ide', 547)]\n",
      "[(23, 'However', 2993)]\n",
      "[(25, 'the', 272)]\n",
      "[(26, 'per', 660), (27, 'ox', 1142), (28, 'ide', 547)]\n",
      "[(29, 'was', 403)]\n",
      "[(30, 'left', 1749)]\n",
      "[(31, 'in', 297)]\n",
      "[(32, 'the', 272)]\n",
      "[(33, 'ear', 8120), (34, 'rings', 26661)]\n",
      "[(35, 'overnight', 22128)]\n",
      "[(36, 'and', 304)]\n",
      "[(37, 'exposed', 13438)]\n",
      "[(38, 'to', 298)]\n",
      "[(39, 'light', 2061)]\n",
      "[(41, 'causing', 13098)]\n",
      "[(42, 'burn', 5698), (43, 's', 28713)]\n",
      "[(44, 'and', 304)]\n",
      "[(45, 'ble', 8012), (46, 'aching', 10028)]\n",
      "[(47, 'on', 356)]\n",
      "[(48, 'the', 272)]\n",
      "[(49, 'person', 1338), (50, \"'\", 28742), (51, 's', 28713)]\n",
      "[(52, 'skin', 4759)]\n",
      "[(54, 'The', 415)]\n",
      "[(55, 'person', 1338)]\n",
      "[(56, 'is', 349)]\n",
      "[(57, 'now', 1055)]\n",
      "[(58, 'experiencing', 20998)]\n",
      "[(59, 'mild', 16583)]\n",
      "[(60, 'pain', 3358)]\n",
      "[(61, 'and', 304)]\n",
      "[(62, 'is', 349)]\n",
      "[(63, 'unable', 9638)]\n",
      "[(64, 'to', 298)]\n",
      "[(65, 'fall', 2949)]\n",
      "[(66, 'asleep', 15231)]\n"
     ]
    }
   ],
   "source": [
    "for summ_word in summary_word_list:\n",
    "    print(summ_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c5bd7650-1740-4ece-8017-73ca295ca5c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(summary_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c17cc8d3-c861-4b2e-bc2f-2bc7fffb2790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  3248,\n",
       "  0,\n",
       "  3248,\n",
       "  0.45301350951194763,\n",
       "  8.643341425340623e-05,\n",
       "  8.798035560175776e-05)]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_token_patching_results[1][:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241e1537-5616-43ff-a0eb-3ba4e0bbdf33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nnsight)",
   "language": "python",
   "name": "nnsight"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
